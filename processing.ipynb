{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import (accuracy_score, classification_report, f1_score,\n",
    "                             precision_score, recall_score)\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from utils.optim import ScheduledOptim\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.summarization import bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1853\n"
     ]
    }
   ],
   "source": [
    "with open('/data/ganleilei/law/ContrastiveLJP/w2id_thulac.pkl', 'rb') as f:\n",
    "    word2id_dict = pickle.load(f)\n",
    "    f.close()\n",
    "print(word2id_dict['无故'])\n",
    "id2word_dict = {item[1]: item[0] for item in word2id_dict.items()}\n",
    "# print(\"word2id dict:\", word2id_dict)\n",
    "# print(\"id2word dict:\", id2word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/data/home/ganleilei/law/ContrastiveLJP/\"\n",
    "train_data, valid_data, test_data = load_dataset(data_path)\n",
    "train_size = len(train_data[\"accu_label_lists\"])\n",
    "accu_labels_set = {}\n",
    "law_labels_set = {}\n",
    "term_labels_set = {}\n",
    "for idx in range(train_size):\n",
    "    accu_label = train_data[\"accu_label_lists\"][idx]\n",
    "    law_label = train_data[\"law_label_lists\"][idx]\n",
    "    term_label = train_data[\"term_lists\"][idx]\n",
    "\n",
    "    if str(accu_label) not in accu_labels_set:\n",
    "        accu_labels_set[str(accu_label)] = 1\n",
    "    else:\n",
    "        accu_labels_set[str(accu_label)] = accu_labels_set[str(accu_label)] + 1\n",
    "\n",
    "    if str(law_label) not in law_labels_set:\n",
    "        law_labels_set[str(law_label)] = 1\n",
    "    else:\n",
    "        law_labels_set[str(law_label)] = law_labels_set[str(law_label)] + 1\n",
    "\n",
    "    if str(term_label) not in term_labels_set:\n",
    "        term_labels_set[str(term_label)] = 1\n",
    "    else:\n",
    "        term_labels_set[str(term_label)] = term_labels_set[str(term_label)] + 1\n",
    "\n",
    "print(\"accu labels set:\", sorted(accu_labels_set.items(), key=lambda k: k[1], reverse=True))\n",
    "print(\"law labels set:\", sorted(law_labels_set.items(), key=lambda k: k[1], reverse=True))\n",
    "print(\"term labels set:\", sorted(term_labels_set.items(), key=lambda k: k[1], reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harnn_res_path = 'harnn_res.csv'\n",
    "res = {}\n",
    "for line in open(harnn_res_path, mode='r'):\n",
    "    parts = line.strip().split()\n",
    "    assert len(parts) == 5, f\"Wrong line, {line}\"\n",
    "    res[str(parts[0])] = tuple(parts[1:])\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_accu_labels_set = {item[0]: item[1] for item in sorted(accu_labels_set.items(), key=lambda k: k[1])}\n",
    "print(sorted_accu_labels_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sorted_accu_labels_set))\n",
    "top = 30\n",
    "p, r, f1 = 0, 0, 0\n",
    "for item in list(sorted_accu_labels_set.items())[119-top:]:\n",
    "    accu_label = item[0]\n",
    "    p += float(res[accu_label][0])\n",
    "    r += float(res[accu_label][1])\n",
    "    f1 += float(res[accu_label][2])\n",
    "\n",
    "print(f\"top {top}, average precision: {p/top}, recall: {r/top}, f1: {f1/top}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(train_data.keys())\n",
    "train_corpus = []\n",
    "mask = np.array(train_data['fact_list']) == 164672\n",
    "mask = ~mask\n",
    "seq_len = mask.sum(2)\n",
    "print(\"seq len:\", seq_len[0])\n",
    "sent_num_mask = seq_len == 0\n",
    "sent_num_mask = ~sent_num_mask\n",
    "sent_num = sent_num_mask.sum(1)\n",
    "print(\"sent num:\", sent_num[0])\n",
    "for s_idx, doc in enumerate(train_data['fact_list']):\n",
    "    tmp = []\n",
    "    cur_sent_num = sent_num[s_idx]\n",
    "    for w_idx, sent in enumerate(doc[:cur_sent_num]):\n",
    "        cur_seq_len = seq_len[s_idx][w_idx]\n",
    "        tmp.extend(sent[:cur_seq_len])\n",
    "\n",
    "    train_corpus.append([id2word_dict[ids] for ids in tmp])\n",
    "\n",
    "print(\"train_corpus size:\", len(train_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(train_corpus)\n",
    "corpus = [dictionary.doc2bow(text) for text in train_corpus]\n",
    "bm25_obj = bm25.BM25(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_facts, test_accu_labels, test_law_labels, test_term_labels = test_data[\"fact_list\"], test_data[\"accu_label_lists\"], test_data[\"law_label_lists\"], test_data[\"term_lists\"]\n",
    "test_corpus = []\n",
    "mask = np.array(test_facts) == 164672\n",
    "mask = ~mask\n",
    "seq_len = mask.sum(2)\n",
    "print(\"seq len:\", seq_len[0])\n",
    "sent_num_mask = seq_len == 0\n",
    "sent_num_mask = ~sent_num_mask\n",
    "sent_num = sent_num_mask.sum(1)\n",
    "print(\"sent num:\", sent_num[0])\n",
    "for s_idx, doc in enumerate(test_facts):\n",
    "    tmp = []\n",
    "    cur_sent_num = sent_num[s_idx]\n",
    "    for w_idx, sent in enumerate(doc[:cur_sent_num]):\n",
    "        cur_seq_len = seq_len[s_idx][w_idx]\n",
    "        tmp.extend(sent[:cur_seq_len])\n",
    "\n",
    "    test_corpus.append([id2word_dict[ids] for ids in tmp])\n",
    "\n",
    "print(\"test_corpus size:\", len(test_corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_index = 21264\n",
    "query_text = test_corpus[query_index]\n",
    "print(\"query text:\", query_text)\n",
    "print(f\"query accu label: {test_accu_labels[query_index]}, law label: {test_law_labels[query_index]}, term label: {test_term_labels[query_index]}\")\n",
    "scores = bm25_obj.get_scores(query_text)\n",
    "best_docs = sorted(range(len(scores)), key=lambda k: scores[k])[-10:]\n",
    "print(\"best docs:\", best_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accu_labels = train_data[\"accu_label_lists\"]\n",
    "train_law_labels = train_data[\"law_label_lists\"]\n",
    "train_term_labels = train_data[\"term_lists\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retr_index = 62983\n",
    "retr_fact = train_corpus[retr_index]\n",
    "print(\"retrieved text:\", retr_fact)\n",
    "print(f\"retrieved accu label: {train_accu_labels[retr_index]}, law label: {train_law_labels[retr_index]}, term label: {train_term_labels[retr_index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"/data/ganleilei/bert/bert-base-chinese/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_texts = bert_tokenizer(\"价值1150元\")\n",
    "print(bert_tokenizer.convert_ids_to_tokens(seg_texts[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "#convert ladan labels index to neurjudge label index\n",
    "neur_judge_charge2id = json.load(open('/data/ganleilei/law/ContrastiveLJP/NeurJudge_config_data/charge2id.json'))\n",
    "ladan_to_neurjudge = {}\n",
    "count = 0\n",
    "with open('/data/ganleilei/workspace/ContrastiveLJP/data/new_big_accu.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if line.strip() in neur_judge_charge2id.keys():\n",
    "            ladan_to_neurjudge[count] = neur_judge_charge2id[line.strip()]\n",
    "        count = count + 1\n",
    "print(ladan_to_neurjudge)\n",
    "\n",
    "neur_judge_law2id = json.load(open('/data/ganleilei/law/ContrastiveLJP/NeurJudge_config_data/article2id.json'))\n",
    "print(neur_judge_law2id)\n",
    "ladan_to_neurjudge_law = {}\n",
    "count = 0\n",
    "with open('/data/ganleilei/workspace/ContrastiveLJP/data/new_big_law.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if line.strip() in neur_judge_law2id.keys():\n",
    "            ladan_to_neurjudge_law[count] = neur_judge_law2id[line.strip()]\n",
    "        count = count + 1\n",
    "print(ladan_to_neurjudge_law)\n",
    "###############################################\n",
    "\n",
    "with open('/data/ganleilei/law/ContrastiveLJP/w2id_thulac.pkl', 'rb') as f:\n",
    "    word2id_dict = pk.load(f)\n",
    "    f.close()\n",
    "print(len(word2id_dict))\n",
    "id2word_dict = {item[1]: item[0] for item in word2id_dict.items()}\n",
    "\n",
    "file_list = [\"train\", \"valid\", \"test\"]\n",
    "#file_list = [\"test\"]\n",
    "for file in file_list:\n",
    "    fact_lists = []\n",
    "    law_label_lists = []\n",
    "    accu_label_lists = []\n",
    "    term_lists = []\n",
    "    ##add for pre-trained models\n",
    "    raw_facts = []\n",
    "\n",
    "    f = pk.load(open('/data/ganleilei/law/ContrastiveLJP/big/{}_processed_thulac_Legal_basis_with_fyb_annotate_number_field.pkl'.format(file), 'rb'))\n",
    "    print(f.keys())\n",
    "    for idx, fact in enumerate(tqdm(f[\"fact_list\"][:100])):\n",
    "        if f['law_label_lists'][idx] not in ladan_to_neurjudge_law or f['accu_label_lists'][idx] not in ladan_to_neurjudge:\n",
    "            continue\n",
    "        print(\"raw fact:\", f['raw_facts_list'][idx])\n",
    "        print(\"law label lists:\", f['law_label_lists'][idx])\n",
    "        print(\"law label lists:\", ladan_to_neurjudge_law[f['law_label_lists'][idx]])\n",
    "\n",
    "        print(\"accu label lists:\", f['accu_label_lists'][idx])\n",
    "        print(\"accu label lists:\", ladan_to_neurjudge[f['accu_label_lists'][idx]])\n",
    "\n",
    "        law_label_lists.append(ladan_to_neurjudge_law[f['law_label_lists'][idx]])\n",
    "        accu_label_lists.append(ladan_to_neurjudge[f['accu_label_lists'][idx]])\n",
    "\n",
    "        sentence = []\n",
    "        for s in fact:\n",
    "            s = s.tolist()\n",
    "            for id in s:\n",
    "                if id != word2id_dict['BLANK']:\n",
    "                    sentence.append(id)\n",
    "        raw_sent = [id2word_dict[id] for id in sentence]\n",
    "        # if \"贩毒\" in raw_sent or \"毒品\" in raw_sent:\n",
    "        #     print(raw_sent)\n",
    "        if len(sentence) < 300:\n",
    "            sentence = sentence + [word2id_dict['BLANK']]*(300-len(sentence))\n",
    "        else:\n",
    "            sentence = sentence[:300]\n",
    "\n",
    "        fact_lists.append(sentence)\n",
    "\n",
    "    data_dict = {'fact_list': fact_lists, 'law_label_lists': law_label_lists,\n",
    "                 'accu_label_lists': accu_label_lists, 'term_lists': f[\"term_lists\"], 'raw_facts_list': f[\"raw_facts_list\"], \n",
    "                 'money_amount_lists': f[\"money_amount_lists\"], 'drug_weight_lists': f[\"drug_weight_lists\"]}\n",
    "\n",
    "    pk.dump(data_dict, open('/data/ganleilei/law/ContrastiveLJP/big/NeurJudge/{}_processed_thulac_Legal_basis_with_fyb_annotate_number_field.pkl.bak'.format(file), 'wb'))\n",
    "    print('{}_dataset is processed over'.format(file)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['生产、销售伪劣产品', '合同诈骗', '诈骗', '持有、使用假币', '行贿', '虚开增值税专用发票、用于骗取出口退税、抵扣税款发票', '侵犯著作权', '信用卡诈骗', '抢夺', '抢劫', '挪用资金', '挪用公款', '故意毁坏财物', '非法吸收公众存款', '集资诈骗', '出售、购买、运输假币', '贷款诈骗', '保险诈骗', '盗窃', '持有伪造的发票', '违法发放贷款', '骗取贷款、票据承兑、金融票证', '非法收购、运输盗伐、滥伐的林木', '对非国家工作人员行贿', '票据诈骗', '职务侵占', '贪污', '走私普通货物、物品', '销售假冒注册商标的商品']\n"
     ]
    }
   ],
   "source": [
    "charges =  [83, 11, 55, 16, 37, 102, 52, 107, 61, 12, 58, 75, 78, 38, 69, 60, 54, 94, 110, 88, 19, 30, 59, 26, 51, 118, 86, 49, 7] # number sensitive classes\n",
    "lines = open('data/new_accu.txt', 'r').readlines()\n",
    "print([lines[t].strip() for t in charges])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def load_dataset(path):\n",
    "    train_path = os.path.join(path, \"train_processed_thulac_Legal_basis_with_fyb_annotate_number_field.pkl\")\n",
    "    valid_path = os.path.join(path, \"valid_processed_thulac_Legal_basis_with_fyb_annotate_number_field.pkl\")\n",
    "    test_path = os.path.join(path, \"test_processed_thulac_Legal_basis_with_fyb_annotate_number_field.pkl\")\n",
    "    \n",
    "    train_dataset = pickle.load(open(train_path, mode='rb'))\n",
    "    valid_dataset = pickle.load(open(valid_path, mode='rb'))\n",
    "    test_dataset = pickle.load(open(test_path, mode='rb'))\n",
    "\n",
    "    print(\"train dataset sample:\", train_dataset['raw_facts_list'][0])\n",
    "    print(\"train dataset sample len:\", len(train_dataset['law_label_lists']))\n",
    "    return train_dataset, valid_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset sample: 2014年4月19日下午16时许，被告人段某驾拖车经过鸡飞乡澡塘街子，时逢堵车，段某将车停在“冰凉一夏”冷饮店门口，被害人王某的侄子王2某示意段某靠边未果，后上前敲打车门让段某离开，段某遂驾车离开，但对此心生怨愤。同年4月21日22时许，被告人段某酒后与其妻子王1某一起准备回家，走到鸡飞乡澡塘街富达通讯手机店门口时停下，段某进入手机店内对被害人王某进行吼骂，紧接着从手机店出来拿得一个石头又冲进手机店内朝王某头部打去，致王某右额部粉碎性骨折、右眼眶骨骨折。经鉴定，被害人王某此次损伤程度为轻伤一级。\n",
      "train dataset sample len: 101619\n",
      "101619\n"
     ]
    }
   ],
   "source": [
    "path = \"/data/ganleilei/law/ContrastiveLJP/datasets/fyb_annotate/\"\n",
    "train_dataset, valid_dataset, test_dataset = load_dataset(path)\n",
    "print(train_dataset['accu_label_lists'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22608, 0.2225\n",
      "37727, 0.3713\n"
     ]
    }
   ],
   "source": [
    "num_charges =  [83, 11, 55, 16, 37, 102, 52, 107, 61, 12, 58, 75, 78, 38, 69, 60, 54, 94, 110, 88, 19, 30, 59, 26, 51, 118, 86, 49, 7] # number sensitive classes\n",
    "conf_target_classes = [1, 3, 5, 6, 11, 12, 15, 18, 22, 24, 25, 26, 27, 30, 33, 38, 42, 44, 45, 48, 54, 55, 61, 68, 69, 74, 77, 78, 79, 82, 86, 91, 93, 100, 105, 108, 110, 111, 112, 113, 118]\n",
    "\n",
    "num_count, conf_count = 0, 0\n",
    "for l in train_dataset['accu_label_lists']:\n",
    "    if l in num_charges:\n",
    "        num_count += 1\n",
    "    if l in conf_target_classes:\n",
    "        conf_count += 1\n",
    "\n",
    "print(\"%d, %.4f\" % (num_count, num_count/101619))\n",
    "print(\"%d, %.4f\" % (conf_count, conf_count/101619))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('lawqa': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0056837d82585e808a6ffa0183bdb5974069eb8c4bb9a2a213d82b07c3a81ac2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
